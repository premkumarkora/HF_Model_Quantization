{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyPXsKVtbJiMrJcZGJMfeCFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/premkumarkora/HF_Model_Quantization/blob/main/HF_Model_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```bash\n",
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate\n",
        "```\n",
        "\n",
        "you have all the core pieces needed to download, run, quantize, and accelerate modern NLP models from HuggingFace—and make HTTP calls too. Here’s what each library gives you, and some of the things you can do immediately afterwards:\n",
        "\n",
        "\n",
        "## 1. `requests`\n",
        "\n",
        "* **What it is:** A simple, human-friendly HTTP client for Python.\n",
        "* **What you can do:**\n",
        "\n",
        "  * Fetch data from web APIs (e.g. pull text from a REST service).\n",
        "  * Download files or model weights if you want to manage HTTP yourself.\n",
        "  * Send results of your model to another service (for example, posting generated text to a web server).\n",
        "\n",
        "\n",
        "## 2. `torch`\n",
        "\n",
        "* **What it is:** PyTorch, the most popular deep-learning framework in Python.\n",
        "* **What you can do:**\n",
        "\n",
        "  * Load and run neural networks (including transformers).\n",
        "  * Write and train your own models, or fine-tune existing ones.\n",
        "  * Manipulate tensors, move them onto GPU, and do low-level operations when needed.\n",
        "\n",
        "\n",
        "## 3. `bitsandbytes`\n",
        "\n",
        "* **What it is:** A library for ultra-memory-efficient model loading and inference via quantization.\n",
        "* **What you can do:**\n",
        "\n",
        "  * Load giant transformer models in 4-bit or 8-bit precision.\n",
        "  * Drastically reduce GPU RAM usage so you can run large models on smaller cards.\n",
        "  * Combine with HuggingFace’s `quantization_config` to trade a tiny bit of accuracy for big memory savings.\n",
        "\n",
        "\n",
        "## 4. `transformers`\n",
        "\n",
        "* **What it is:** HuggingFace’s flagship library for all things transformer-based NLP (and beyond).\n",
        "* **What you can do:**\n",
        "\n",
        "  * Download thousands of pre-trained models (GPT, BERT, T5, BLOOM, etc.) with a single line.\n",
        "  * Tokenize text, run model inference, and decode outputs back to human text.\n",
        "  * Use pipelines for common tasks (text generation, summarization, translation, question answering) in just a couple lines of code.\n",
        "\n",
        "\n",
        "## 5. `sentencepiece`\n",
        "\n",
        "* **What it is:** A fast, language-agnostic tokenizer library often used under the hood by big models.\n",
        "* **What you can do:**\n",
        "\n",
        "  * Tokenize or detokenize your own text in the same way models like T5 or mT5 do.\n",
        "  * Train new subword tokenizers if you have domain-specific text.\n",
        "  * Ensure compatibility when you load models that explicitly require SentencePiece (e.g. certain multilingual or translation models).\n",
        "\n",
        "\n",
        "## 6. `accelerate`\n",
        "\n",
        "* **What it is:** HuggingFace’s lightweight tool for easily running code on CPU, single GPU, or multi-GPU setups.\n",
        "* **What you can do:**\n",
        "\n",
        "  * Spin up your training or inference with zero boilerplate for distributed/parallel setups.\n",
        "  * Automatically move your model and data to the right devices.\n",
        "  * Scale from your laptop’s CPU to a multi-GPU server by changing one command-line flag.\n",
        "\n",
        "\n",
        "With these six packages in place, you’re set to explore or build anything from simple demos to large-scale, memory-efficient NLP services. Enjoy!\n"
      ],
      "metadata": {
        "id": "dGwtOin7Xv19"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHt733o8XQuF"
      },
      "outputs": [],
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc"
      ],
      "metadata": {
        "id": "bpbsEF5XZMQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes"
      ],
      "metadata": {
        "id": "3k3l3Qy73U3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sign in to Hugging Face"
      ],
      "metadata": {
        "id": "zsIgG_x_ZPV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "QkJ4WnTcZPyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models that are goint to be used in this notebook"
      ],
      "metadata": {
        "id": "A860yJ9pZUJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instruct models\n",
        "\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\"\n",
        "QWEN2 = \"Qwen/Qwen2-7B-Instruct\" # exercise for you\n",
        "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # If this doesn't fit it your GPU memory, try others from the hub"
      ],
      "metadata": {
        "id": "gZCq2Et-ZTF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a message template"
      ],
      "metadata": {
        "id": "di358innZhY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of ship captains\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "NeiMIjdyZhJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Quantize the Model\n",
        "\n",
        "\n",
        "The `BitsAndBytesConfig` defined tells HuggingFace’s loading logic how to quantize your model down to 4-bit weights. Quantization lets you squeeze a large model into GPU memory (or even CPU RAM) by storing its weights in fewer bits—at the cost of (usually very small) drops in numerical precision. Here’s what each argument does:\n",
        "\n",
        "\n",
        "\n",
        "### 1. `load_in_4bit=True`\n",
        "\n",
        "* **What it does:**\n",
        "  Instructs the loader to convert every weight tensor from 16- or 32-bit floats down to 4 bits per value as you load the model.\n",
        "\n",
        "  A 4 bit weight uses one-eighth the memory of a 32 bit weight. For a 10 billion-parameter model, that’s a drop from \\~40 GB down to \\~5 GB—often the difference between “fits on one consumer GPU” or “doesn’t.”\n",
        "\n",
        "\n",
        "\n",
        "### 2. `bnb_4bit_use_double_quant=True`\n",
        "\n",
        "* **What it does:**\n",
        "  Applies a two-step (“double”) quantization:\n",
        "\n",
        "  1. **Primary quantization** reduces each block of weights to 4 bits.\n",
        "  2. **Secondary quantization** further compresses the scale factors themselves (the small floating-point numbers that map 4-bit integers back to approximate original floats).\n",
        "\n",
        "  You get even smaller overall memory footprint without materially worsening accuracy. Double-quantized scale factors take less space, but you still recover enough precision to generate sensible outputs.\n",
        "\n",
        "\n",
        "\n",
        "### 3. `bnb_4bit_compute_dtype=torch.bfloat16`\n",
        "\n",
        "\n",
        "  Chooses the data-type for all *compute* (matrix multiplications, attention, etc.) once the 4 bit model has been de-quantized on the fly.\n",
        "\n",
        "* **Why **`bfloat16`**?**\n",
        "\n",
        "  * **bfloat16** (Brain Floating Point) is a 16-bit format with the same exponent range as 32-bit floats, but fewer mantissa bits.\n",
        "  * It keeps dynamic range high (so very large and very small numbers behave well) while still cutting memory in half versus full 32 bit.\n",
        "\n",
        "* **Result:**\n",
        "  When you actually run the model, tensors are cast to `bfloat16` for speed and moderate precision, rather than back to full `float32` which costs more memory and compute.\n",
        "\n",
        "\n",
        "### 4. `bnb_4bit_quant_type=\"nf4\"`\n",
        "\n",
        "* **What it does:**\n",
        "  Chooses which 4-bit quantization scheme to use. `\"nf4\"` stands for **NormalFloat-4**, a recent format developed to give better accuracy than simple integer quantization.\n",
        "* **How it works at a high level:**\n",
        "\n",
        "  * **NF4** stores 4 bit values not as straight 0–15 integers, but as tiny floating-point representations with a tiny shared exponent per block.\n",
        "  * This lets very small and very large weights both be encoded more faithfully than pure linear quantization.\n",
        "\n",
        "  In practice, models quantized with NF4 lose less performance (measured in perplexity or downstream task scores) than ones quantized with uniform 4 bit.\n"
      ],
      "metadata": {
        "id": "D-0z5gyZZzd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "fFQO97EaZ3Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLAMA"
      ],
      "metadata": {
        "id": "yymLoUXOar1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "I1JfG2blasJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization Configuration"
      ],
      "metadata": {
        "id": "e_CH5N1vbyRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
      ],
      "metadata": {
        "id": "-mdXKIyTbysU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MPf9YrqndoxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = model.get_memory_footprint() / 1e6\n",
        "print(f\"Memory footprint: {memory:,.1f} MB\")"
      ],
      "metadata": {
        "id": "sdLgtdTsdpX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looking under the hood at the Transformer model\n",
        "The next cell prints the HuggingFace model object for Llama.\n",
        "\n",
        "This model object is a Neural Network, implemented with the Python framework PyTorch. The Neural Network uses the architecture invented by Google scientists in 2017: the Transformer architecture.\n",
        "\n",
        "While we're not going to go deep into the theory, this is an opportunity to get some intuition for what the Transformer actually is.\n",
        "\n",
        "If you're completely new to Neural Networks, check out my YouTube intro playlist for the foundations.\n",
        "\n",
        "Now take a look at the layers of the Neural Network that get printed in the next cell. Look out for this:\n",
        "\n",
        "It consists of layers\n",
        "There's something called \"embedding\" - this takes tokens and turns them into 4,096 dimensional vectors. We'll learn more about this in Week 5.\n",
        "There are then 32 sets of groups of layers called \"Decoder layers\". Each Decoder layer contains three types of layer: (a) self-attention layers (b) multi-layer perceptron (MLP) layers (c) batch norm layers.\n",
        "There is an LM Head layer at the end; this produces the output\n",
        "Notice the mention that the model has been quantized to 4 bits.\n",
        "\n",
        "It's not required to go any deeper into the theory at this point, but if you'd like to, I've asked our mutual friend to take this printout and make a tutorial to walk through each layer. This also looks at the dimensions at each point. If you're interested, work through this tutorial after running the next cell:\n",
        "\n",
        "https://chatgpt.com/canvas/shared/680cbea6de688191a20f350a2293c76b"
      ],
      "metadata": {
        "id": "PvMf-fuEd-PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "SX947oJ8eAt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(inputs, max_new_tokens=80)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "6vvCDLyIejc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up memory\n",
        "# Thank you Kuan L. for helping me get this to properly free up memory!\n",
        "# If you select \"Show Resources\" on the top right to see GPU memory, it might not drop down right away\n",
        "# But it does seem that the memory is available for use by new models in the later code.\n",
        "\n",
        "del model, inputs, tokenizer, outputs\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "gjuyYvwNgcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ptrnhltvfYVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Super Heros\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "z4UKMcxdfqOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, messages):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
        "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
        "  memory = model.get_memory_footprint() / 1e6\n",
        "  print(f\"Memory footprint: {memory:,.1f} MB\")\n",
        "  #del model, inputs, tokenizer, outputs, streamer\n",
        "  #gc.collect()\n",
        "  #torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kFoPaMAhfYqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to quantize and load the model back to Hunning Face"
      ],
      "metadata": {
        "id": "MUyg108ykHWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"google/gemma-2-2b-it\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()                # enter your HF token\n",
        "model.push_to_hub(\"premkumarkora/gemma-2-2b-it\")\n",
        "tokenizer.push_to_hub(\"premkumarkora/gemma-2-2b-it\")"
      ],
      "metadata": {
        "id": "aEEjdPUUjH5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(GEMMA2, messages)"
      ],
      "metadata": {
        "id": "L01hwEvefa2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up memory\n",
        "# Thank you Kuan L. for helping me get this to properly free up memory!\n",
        "# If you select \"Show Resources\" on the top right to see GPU memory, it might not drop down right away\n",
        "# But it does seem that the memory is available for use by new models in the later code.\n",
        "\n",
        "del model, inputs, tokenizer, outputs\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "T7AD5_pngkS3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}